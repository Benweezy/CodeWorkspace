{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentiles(df, athlete_id, metric_dict, selected_metrics=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate percentile rankings for an athlete compared to their peers, grouped by competition level.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset containing test data.\n",
    "        athlete_id (int): The ID of the athlete.\n",
    "        metric_dict (dict): Dictionary defining metrics to analyze.\n",
    "        selected_metrics (list, optional): Specific metrics to process. Defaults to all metrics in metric_dict.\n",
    "        verbose (bool): Whether to print detailed processing information.\n",
    "\n",
    "    Returns:\n",
    "        dict: Percentiles calculated for each metric.\n",
    "        str: Athlete's name.\n",
    "        str: Athlete's level.\n",
    "    \"\"\"\n",
    "    # Initialize percentiles and athlete details\n",
    "    percentiles = {}\n",
    "    athlete_data = df[df['ID'] == athlete_id]\n",
    "\n",
    "    # Check if athlete data exists\n",
    "    if athlete_data.empty:\n",
    "        if verbose:\n",
    "            print(f\"No data found for athlete ID {athlete_id}.\")\n",
    "        return {}, None, None\n",
    "\n",
    "    # Get athlete's name and current level\n",
    "    athlete_name = f\"{athlete_data['First Name'].iloc[0]} {athlete_data['Last Name'].iloc[0]}\"\n",
    "    athlete_level = athlete_data['Level'].iloc[-1]  # Use the most recent level\n",
    "\n",
    "    # Filter data for the athlete's current level\n",
    "    peer_data = df[df['Level'] == athlete_level]\n",
    "    if peer_data.empty:\n",
    "        if verbose:\n",
    "            print(f\"No peer data found for level '{athlete_level}'.\")\n",
    "        return {}, athlete_name, athlete_level\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Processing data for {athlete_name} at level '{athlete_level}' with {len(peer_data)} peers.\")\n",
    "\n",
    "    # Process metrics\n",
    "    for (test_type, sub_type), metrics in metric_dict.items():\n",
    "        for metric in metrics:\n",
    "            # Skip if not in selected metrics\n",
    "            if selected_metrics and metric not in selected_metrics:\n",
    "                continue\n",
    "\n",
    "            subset = peer_data[(peer_data['Test Type'] == test_type) &\n",
    "                               (peer_data['Test Sub-Type'] == sub_type)]\n",
    "\n",
    "            if metric not in subset.columns:\n",
    "                if verbose:\n",
    "                    print(f\"Metric '{metric}' not found in data for {test_type} - {sub_type}\")\n",
    "                continue\n",
    "\n",
    "            # Convert to numeric and drop NaN\n",
    "            subset[metric] = pd.to_numeric(subset[metric], errors='coerce')\n",
    "            subset = subset.dropna(subset=[metric])\n",
    "\n",
    "            # Skip if subset is empty\n",
    "            if subset.empty:\n",
    "                if verbose:\n",
    "                    print(f\"No valid data for metric '{metric}' under {test_type} - {sub_type}.\")\n",
    "                continue\n",
    "\n",
    "            athlete_subset = subset[subset['ID'] == athlete_id].sort_values(by='Test Number')\n",
    "\n",
    "            # Handle cases with fewer than two tests\n",
    "            if athlete_subset.empty:\n",
    "                if verbose:\n",
    "                    print(f\"No test data available for metric '{metric}' for athlete ID {athlete_id}.\")\n",
    "                continue\n",
    "\n",
    "            # Extract test values\n",
    "            first_test = athlete_subset.iloc[0][metric]  # First test value\n",
    "            most_recent_test = athlete_subset.iloc[-1][metric]  # Most recent test value\n",
    "            previous_test = athlete_subset.iloc[-2][metric] if len(athlete_subset) > 1 else first_test\n",
    "\n",
    "            # Ensure no NaN values are included in calculations\n",
    "            first_test = first_test if pd.notna(first_test) else 0\n",
    "            most_recent_test = most_recent_test if pd.notna(most_recent_test) else 0\n",
    "            previous_test = previous_test if pd.notna(previous_test) else 0\n",
    "\n",
    "            # Calculate percentile ranks\n",
    "            first_percentile = percentileofscore(subset[metric], first_test)\n",
    "            recent_percentile = percentileofscore(subset[metric], most_recent_test)\n",
    "            previous_percentile = percentileofscore(subset[metric], previous_test)\n",
    "\n",
    "            # Store results\n",
    "            label = f\"{test_type} - {sub_type} - {metric}\"\n",
    "            percentiles[label] = {\n",
    "                'first_percentile': first_percentile,\n",
    "                'previous_percentile': previous_percentile,\n",
    "                'recent_percentile': recent_percentile,\n",
    "                'first_value': first_test,\n",
    "                'previous_value': previous_test,\n",
    "                'recent_value': most_recent_test\n",
    "            }\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Calculated percentiles for {label}: {percentiles[label]}\")\n",
    "\n",
    "    return percentiles, athlete_name, athlete_level\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
